{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "from afinn import Afinn\n",
    "#tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "emotion_lines = open('dialogues_emotion.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('dialogues_text.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "act_lines = open('dialogues_act.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentences that we will be using to train our model.\n",
    "#lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentences' ids, which will be processed to become our input and target data.\n",
    "#conv_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-530b77b8874e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a dictionary to map each line's id with its text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mid2line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' +++$+++ '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hekitchenstinks.__eou__Illthrowoutthegarbage.__eou_'],\n",
       " ['oDick',\n",
       "  'howaboutgettingsomecoffeefortonight?__eou__Coffee?Idon’thonestlylikethatkindofstuff.__eou__Comeon',\n",
       "  'youcanatleasttryalittle',\n",
       "  'besidesyourcigarette.__eou__What’swrongwiththat?CigaretteisthethingIgocrazyfor.__eou__Notforme',\n",
       "  'Dick.__eou_'],\n",
       " ['rethingsstillgoingbadlywithyourhouseguest?__eou__Gettingworse.Nowhe’seatingmeoutofhouseandhome.I’Vetriedtalkingtohimbutitallgoesinoneearandouttheother.Hemakeshimselfathome',\n",
       "  'whichisfine.ButwhatreallygetsmeisthatyesterdayhewalkedintothelivingroomintherawandIhadcompanyover!Thatwasthelaststraw.__eou__Leo',\n",
       "  'Ireallythinkyou’rebeatingaroundthebushwiththisguy.Iknowheusedtobeyourbestfriendincollege',\n",
       "  'butIreallythinkit’stimetolaydownthelaw.__eou__You’reright.Everythingisprobablygoingtocometoaheadtonight.I’llkeepyouinformed.__eou_'],\n",
       " ['ouldyoumindwaitingawhile?__eou__Well',\n",
       "  'howlongwillitbe?__eou__Imnotsure.ButIllgetatablereadyasfastasIcan.__eou__OK.Wellwait.__eou_'],\n",
       " ['reyougoingtotheannualparty?Icangiveyouarideifyouneedone.__eou__Thanksalot.ThatsthefavorIwasgoingtoaskyoufor.__eou__Thepleasureismine.__eou_'],\n",
       " ['sn’thethebestinstructor?Ithinkhe’ssohot.Wow!Ireallyfeelenergized',\n",
       "  'don’tyou?__eou__Iswear',\n",
       "  'I’mgoingtokillyouforthis.__eou__What’swrong?Didn’tyouthinkitwasfun?!__eou__Oh',\n",
       "  'yeah!Ihadablast!Ilovesweatinglikeapigwithabunchofpotbellieswhoallsmellbad.Sorry',\n",
       "  'I’mjustnotintothishealthkick.__eou__Oh',\n",
       "  'no',\n",
       "  'getoffit.Itwasn’tsuchakillerclass.Youjusthavetogetintoit.Liketheysay',\n",
       "  'nopain',\n",
       "  'nogain.__eou__Iamwipedout.Thankyou.__eou__Look',\n",
       "  'nexttimegetyourselfsomecomfyshoes.You’regonnacomebackagainwithme',\n",
       "  'aren’tyou?__eou__Never!Butthankyouforinvitingme.__eou__Comeon.You’llfeelbetterafterwehittheshowers.__eou_'],\n",
       " ['anItakeyourordernowordoyoustillwanttolookatthemenu?__eou__Well',\n",
       "  'Iwantafilletsteak',\n",
       "  'medium',\n",
       "  'butmylittlegirldoesntcareforsteak.Couldshehavesomethingelseinstead?__eou__Certainly.Howaboutspaghettiwithclamsandshrimps.__eou__Soundsdelicious.OK.Shelltrythat.__eou_'],\n",
       " ['anyoumanagechopsticks?__eou__Whynot?See.__eou__Goodmastery.HowdoyoulikeourChinesefood?__eou__Oh',\n",
       "  'great!Itsdelicious.Yousee',\n",
       "  'Iamalreadyputtingonweight.ThereisonethingIdontlikehowever',\n",
       "  'MSG.__eou__WhatswrongwithMSG?Ithelpstobringoutthetasteofthefood.__eou__Accordingtosomestudiesitmaycausecancer.__eou__Oh',\n",
       "  'dontletthatworryyou.Ifthatweretrue',\n",
       "  'Chinawouldnthavesuchalargepopulation.__eou__Ijusthappentohaveaquestionforyouguys.WhydotheChinesecookthevegetables?YouseewhatImeanisthatmostvitaminaredestroyedwhenheated.__eou__Idontknowexactly.Itsatradition.Maybeitsforsanitaryreasons.__eou_'],\n",
       " ['mexhausted.__eou__Okay', 'letsgohome.__eou_'],\n",
       " ['oodevening.WelcometoCherrys.Doyouhaveareservation?__eou__No',\n",
       "  'wedont.__eou__Howmanyofyou',\n",
       "  'please?__eou__Six',\n",
       "  'includingtwokids.__eou__Imafraidallthebigtablesaretaken.__eou_']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "convs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "emotion_lines = open('dialogues_emotion.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "act_lines = open('dialogues_act.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "text_lines = open('dialogues_text.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "emotions =[] \n",
    "for i in range(len(emotion_lines)):\n",
    "    emotions.append(emotion_lines[i].split(\" \"))\n",
    "\n",
    "act = []\n",
    "for i in range(len(act_lines)):\n",
    "    act.append(act_lines[i].split(\" \"))\n",
    "\n",
    "corpus = [] \n",
    "for i in range(len(text_lines)):\n",
    "    corpus.append(text_lines[i].split(\"__eou__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '0', '']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The kitchen stinks . ', \" I'll throw out the garbage . \", '']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8636\n",
      "8636\n"
     ]
    }
   ],
   "source": [
    "new_questions = []\n",
    "new_answers = []\n",
    "afinn = Afinn(language='en')  \n",
    "\n",
    "len(corpus)-1\n",
    "for j in range(len(corpus)-1):\n",
    "    for i in range(len(corpus[j])-2):\n",
    "        if(emotions[j][i] == '4'):\n",
    "            if afinn.score(corpus[j][i+1]) >= 0:\n",
    "                new_questions.append(corpus[j][i])\n",
    "                new_answers.append(corpus[j][i+1])\n",
    "                \n",
    "\n",
    "print(len(new_questions))   \n",
    "print(len(new_answers))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # to process standard python strings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_response(questions,answers,user_input):\\n    \\n    questions.append(user_input)\\n    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\\n    tfidf = TfidfVec.fit_transform(questions)\\n\\n    \\n    vals = cosine_similarity(tfidf[-1], tfidf)\\n    idx=vals.argsort()[0][-2]\\n    \\n    flat = vals.flatten()\\n    flat.sort()\\n    req_tfidf = flat[-1]\\n    \\n    if(req_tfidf==0):\\n            robo_response = \"I am sorry! I don\\'t understand you\"\\n    else:\\n            robo_response = answers[idx]\\n            \\n    return robo_response\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def generate_response(questions,answers,user_input):\n",
    "    \n",
    "    questions.append(user_input)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n",
    "    tfidf = TfidfVec.fit_transform(questions)\n",
    "\n",
    "    \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    \n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-1]\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "            robo_response = \"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "            robo_response = answers[idx]\n",
    "            \n",
    "    return robo_response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(questions,answers,user_input):\n",
    "    \n",
    "    #print(len(questions))\n",
    "    #print(len(answers))\n",
    "    questions.append(user_input)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n",
    "    tfidf = TfidfVec.fit_transform(questions)\n",
    "\n",
    "    \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    \n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    question_response = [questions[idx], questions[vals.argsort()[0][-3]],questions[vals.argsort()[0][-4]],\n",
    "                    questions[vals.argsort()[0][-5]],questions[vals.argsort()[0][-6]]]\n",
    "    robo_response = [answers[idx], answers[vals.argsort()[0][-3]],answers[vals.argsort()[0][-4]],\n",
    "                    answers[vals.argsort()[0][-5]],answers[vals.argsort()[0][-6]]]\n",
    "    \"\"\"\n",
    "    if(req_tfidf==0):\n",
    "            robo_response = \"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "            #print(idx, \"idex\")\n",
    "            robo_response = answers[idx]\n",
    "    \n",
    "    answers.append(robo_response)\n",
    "    #return question_response, robo_response\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: \n",
      "HI\n",
      "Robo: \n",
      " Yes . I moved here last week . \n",
      "User: \n"
     ]
    }
   ],
   "source": [
    "quit = False\n",
    "\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
    "GREETING_RESPONSES = \"Hello Cutie\"\n",
    "\n",
    "while quit == False:\n",
    "    print(\"User: \")\n",
    "    user_input = input()\n",
    "    print(\"Robo: \")\n",
    "    if(user_input in GREETING_INPUTS):\n",
    "        print(GREETING_RESPONSES)\n",
    "    elif(user_input!='bye'):\n",
    "        print(generate_response(new_questions,new_answers,user_input))\n",
    "    else:\n",
    "        print(\"bye\")\n",
    "        quit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('frames.json', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "    \n",
    "# parse file\n",
    "obj = json.loads(data)\n",
    "\n",
    "# show values\n",
    "print(obj[100].get(\"text\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
